{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb18b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, Model, callbacks\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.kde import KDE\n",
    "from scipy.stats import kurtosis, skew, chi2, entropy\n",
    "\n",
    "class T2Detector:\n",
    "    \"\"\"Hotelling's T-squared multivariate statistical control detector.\"\"\"\n",
    "    def __init__(self, alpha=0.005):\n",
    "        self.alpha = alpha\n",
    "        self.mean_ = None\n",
    "        self.cov_ = None\n",
    "        self.inv_cov_ = None\n",
    "        self.threshold_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.cov_ = np.cov(X, rowvar=False)\n",
    "        if np.ndim(self.cov_) == 0:\n",
    "            self.cov_ = np.array([[self.cov_]])\n",
    "        self.inv_cov_ = np.linalg.pinv(self.cov_)\n",
    "        df = X.shape[1]\n",
    "        self.threshold_ = chi2.ppf(1 - self.alpha, df=df)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        diff = X - self.mean_\n",
    "        md2 = np.sum(diff @ self.inv_cov_ * diff, axis=1)\n",
    "        return (md2 > self.threshold_).astype(int)\n",
    "\n",
    "class BoxplotOutlier1D:\n",
    "    \"\"\"1D boxplot outlier detector (IQR method).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X)\n",
    "        x = X[:, 0] if (X.ndim == 2 and X.shape[1] == 1) else X.ravel()\n",
    "        q1, q3 = np.percentile(x, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        self.low_ = q1 - 1.5 * iqr\n",
    "        self.high_ = q3 + 1.5 * iqr\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        x = X[:, 0] if (X.ndim == 2 and X.shape[1] == 1) else X.ravel()\n",
    "        out = (x < self.low_) | (x > self.high_)\n",
    "        return out.astype(int)\n",
    "\n",
    "class VSCOUT:\n",
    "    \"\"\"\n",
    "    Variational Self-Correcting Outlier Uncovering Technique (VSCOUT).\n",
    "    Combines ARD-VAE, PELT changepoint detection, and ensemble methods for anomaly detection.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_neurons=(64,), decoder_neurons=(64,), latent_dim=32,\n",
    "        learning_rate=1e-4, alpha=0.005, penalty=40, kl_threshold=1,\n",
    "        flag_rule=\"any\", n_jobs=1, kurtosis_threshold=5.0\n",
    "    ):\n",
    "        self.encoder_neurons = encoder_neurons\n",
    "        self.decoder_neurons = decoder_neurons\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.penalty = penalty\n",
    "        self.kl_threshold = kl_threshold\n",
    "        self.flag_rule = flag_rule\n",
    "        self.n_jobs = n_jobs\n",
    "        self.kurtosis_threshold = kurtosis_threshold\n",
    "\n",
    "        self.vae = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.latent_encoder = None\n",
    "        self.orig_dim = None\n",
    "        self._X_train = None\n",
    "        self.z_inliers = None\n",
    "        self.change_points = None\n",
    "        self.inlier_mask = None\n",
    "        self.kl_divs = None\n",
    "        self.relevant_latents = None\n",
    "        self.ere1_threshold = None\n",
    "        self.latent_mean_inlier = None\n",
    "        self.latent_cov_inlier = None\n",
    "        self.t2_threshold = None\n",
    "        self.base_detectors = None\n",
    "\n",
    "    @staticmethod\n",
    "    def suggest_flag_rule(X):\n",
    "        \"\"\"Analyzes data distribution to suggest 'any' or 'majority' voting for the ensemble.\"\"\"\n",
    "        X = np.nan_to_num(np.asarray(X))\n",
    "        avg_kurt = np.mean(np.abs(kurtosis(X, axis=0, fisher=False)))\n",
    "        avg_skew = np.mean(np.abs(skew(X, axis=0)))\n",
    "        n_samples, n_features = X.shape\n",
    "        entropies = [entropy(np.histogram(X[:,i], bins=10)[0] + 1e-8) for i in range(n_features)]\n",
    "        avg_entropy = np.mean(entropies)\n",
    "        \n",
    "        multimodal = False\n",
    "        try:\n",
    "            from scipy.signal import find_peaks\n",
    "            for i in range(n_features):\n",
    "                counts, _ = np.histogram(X[:,i], bins=20)\n",
    "                peaks, props = find_peaks(counts, height=np.max(counts)*0.4, prominence=np.max(counts)*0.3, distance=2)\n",
    "                if len(peaks) > 1:\n",
    "                    sorted_h = np.sort(props['peak_heights'])\n",
    "                    if sorted_h[-2] > 0.5 * sorted_h[-1]:\n",
    "                        multimodal = True\n",
    "                        break\n",
    "        except Exception: pass\n",
    "\n",
    "        if avg_kurt > 5 or avg_skew > 5 or avg_entropy > 2.0:\n",
    "            return \"any\"\n",
    "        return \"majority\"\n",
    "\n",
    "    def _build_model(self, X, force_structure=False):\n",
    "        if self.flag_rule is None:\n",
    "            self.flag_rule = self.suggest_flag_rule(X)\n",
    "\n",
    "        enc_in = layers.Input(shape=(X.shape[1],), name=\"enc_in\")\n",
    "        h = enc_in\n",
    "        for neurons in self.encoder_neurons:\n",
    "            h = layers.Dense(neurons, activation=\"relu\", kernel_initializer=\"he_normal\")(h)\n",
    "            h = layers.BatchNormalization()(h)\n",
    "            h = layers.Dropout(0.2)(h)\n",
    "        \n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(h)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(h)\n",
    "\n",
    "        def sample(args):\n",
    "            m, lv = args\n",
    "            return m + tf.exp(0.5 * lv) * tf.random.normal(tf.shape(m))\n",
    "        \n",
    "        z = layers.Lambda(sample, name=\"z\")([z_mean, z_log_var])\n",
    "        self.encoder = Model(enc_in, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "        lat_in = layers.Input(shape=(self.latent_dim,), name=\"z_sampling\")\n",
    "        h2 = lat_in\n",
    "        for neurons in self.decoder_neurons:\n",
    "            h2 = layers.Dense(neurons, activation=\"relu\", kernel_initializer=\"he_normal\")(h2)\n",
    "            h2 = layers.BatchNormalization()(h2)\n",
    "            h2 = layers.Dropout(0.2)(h2)\n",
    "        \n",
    "        dec_out = layers.Dense(X.shape[1], activation=\"linear\")(h2)\n",
    "        self.decoder = Model(lat_in, dec_out, name=\"decoder\")\n",
    "\n",
    "        class ARDVAE(Model):\n",
    "            def __init__(self, encoder, decoder):\n",
    "                super().__init__()\n",
    "                self.encoder, self.decoder = encoder, decoder\n",
    "                self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "                self.recon_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "                self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "            def call(self, inputs, training=False):\n",
    "                _, _, z = self.encoder(inputs, training=training)\n",
    "                return self.decoder(z, training=training)\n",
    "\n",
    "            @property\n",
    "            def metrics(self):\n",
    "                return [self.total_loss_tracker, self.recon_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "            def train_step(self, data):\n",
    "                x = data[0] if isinstance(data, tuple) else data\n",
    "                with tf.GradientTape() as tape:\n",
    "                    z_mean, z_log_var, z = self.encoder(x, training=True)\n",
    "                    x_recon = self.decoder(z, training=True)\n",
    "                    recon_loss = tf.reduce_sum(tf.square(x - x_recon), axis=1)\n",
    "                    kl_loss = 0.5 * tf.reduce_sum(-1 - z_log_var + tf.square(z_mean) + tf.exp(z_log_var), axis=1)\n",
    "                    total_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "                \n",
    "                grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "                self.total_loss_tracker.update_state(total_loss)\n",
    "                self.recon_loss_tracker.update_state(tf.reduce_mean(recon_loss))\n",
    "                self.kl_loss_tracker.update_state(tf.reduce_mean(kl_loss))\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "            def compute_latent_statistics(self, z_means, inlier_mask):\n",
    "                z_in = z_means[inlier_mask]\n",
    "                self.latent_mean_normal = np.median(z_in, axis=0)\n",
    "                self.latent_var_normal = (np.median(np.abs(z_in - self.latent_mean_normal), axis=0) + 1e-6) ** 2\n",
    "\n",
    "        self.vae = ARDVAE(self.encoder, self.decoder)\n",
    "        self.latent_encoder = Model(self.encoder.input, self.encoder.get_layer(\"z_mean\").output)\n",
    "\n",
    "    def _fit_ensemble(self, z_mean_relevant):\n",
    "        z = np.asarray(z_mean_relevant)\n",
    "        detectors = []\n",
    "        if z.ndim == 2 and z.shape[1] == 1:\n",
    "            detectors.append(BoxplotOutlier1D())\n",
    "        \n",
    "        detectors.extend([\n",
    "            IForest(), LOF(), HBOS(), ECOD(), KNN(n_neighbors=2), KDE(),\n",
    "            T2Detector(alpha=self.alpha)\n",
    "        ])\n",
    "        for clf in detectors:\n",
    "            clf.fit(z_mean_relevant)\n",
    "        self.base_detectors = detectors\n",
    "\n",
    "    def _ensemble_predict(self, z_mean_relevant, rule='majority'):\n",
    "        base_preds = np.array([clf.predict(z_mean_relevant) for clf in self.base_detectors])\n",
    "        votes = np.sum(base_preds, axis=0)\n",
    "        if rule == 'majority':\n",
    "            return votes >= (len(self.base_detectors) // 2 + 1)\n",
    "        return np.any(base_preds, axis=0)\n",
    "\n",
    "    def fit(self, X_train, epochs=30, batch_size=32, verbose=1):\n",
    "        \"\"\"Fits the model using a two-stage approach to filter outliers before final training.\"\"\"\n",
    "        self.orig_dim = X_train.shape[1]\n",
    "        self._X_train = X_train.copy()\n",
    "        self._build_model(X_train)\n",
    "        \n",
    "        early_stop = callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "        self.vae.compile(optimizer=tf.keras.optimizers.Adam(self.learning_rate))\n",
    "        \n",
    "        # Stage 1: Initial training to identify noise/changepoints\n",
    "        self.vae.fit(X_train, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[early_stop])\n",
    "        \n",
    "        initial_enc_w = self.encoder.get_weights()\n",
    "        initial_dec_w = self.decoder.get_weights()\n",
    "        \n",
    "        z_mean, z_log_var, _ = self.encoder.predict(X_train, batch_size=batch_size)\n",
    "        kl_divs = np.mean(0.5 * (-1 - z_log_var + np.square(z_mean) + np.exp(z_log_var)), axis=0)\n",
    "        relevant_indices = np.where(kl_divs > self.kl_threshold)[0]\n",
    "        self.relevant_latents = relevant_indices if len(relevant_indices) > 0 else np.array([0])\n",
    "        \n",
    "        z_relevant = z_mean[:, self.relevant_latents]\n",
    "        l2_norm = np.linalg.norm(z_relevant, axis=1)\n",
    "        self.change_points = rpt.Pelt(model=\"rbf\").fit(l2_norm.reshape(-1, 1)).predict(pen=self.penalty)\n",
    "        \n",
    "        cp_mask = np.zeros(len(z_mean), dtype=bool)\n",
    "        if len(self.change_points) > 0:\n",
    "            cp_mask[self.change_points[0]:] = True\n",
    "            \n",
    "        self._fit_ensemble(z_relevant)\n",
    "        suod_mask = self._ensemble_predict(z_relevant, rule=self.flag_rule)\n",
    "        \n",
    "        self.inlier_mask = ~(np.logical_or(cp_mask, suod_mask))\n",
    "        X_inliers = X_train[self.inlier_mask]\n",
    "        \n",
    "        # Stage 2: Refined training on inliers\n",
    "        self._build_model(X_inliers, force_structure=True)\n",
    "        self.vae.compile(optimizer=tf.keras.optimizers.Adam(self.learning_rate))\n",
    "        self.vae.fit(X_inliers, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[early_stop])\n",
    "        \n",
    "        z_in_mean, _, _ = self.encoder.predict(X_inliers, batch_size=batch_size)\n",
    "        self.z_inliers = z_in_mean[:, self.relevant_latents]\n",
    "        self.latent_mean_inlier = np.mean(self.z_inliers, axis=0)\n",
    "        \n",
    "        cov = np.cov(self.z_inliers, rowvar=False)\n",
    "        self.latent_cov_inlier = np.array([[cov]]) if np.ndim(cov) == 0 else cov\n",
    "        self.t2_threshold = chi2.ppf(1 - self.alpha, df=len(self.relevant_latents))\n",
    "        self._fit_ensemble(self.z_inliers)\n",
    "\n",
    "    def is_outlier(self, data, batch_size=32):\n",
    "        \"\"\"Returns outlier masks based on ensemble, T2, changepoints, and reconstruction error.\"\"\"\n",
    "        z_mean, z_log_var, z_sample = self.encoder.predict(data, batch_size=batch_size)\n",
    "        z_rel = z_mean[:, self.relevant_latents]\n",
    "\n",
    "        cp_mask = np.zeros(len(z_rel), dtype=bool)\n",
    "        if self.change_points and len(self.change_points) > 0:\n",
    "            cp_mask[self.change_points[0]:] = True\n",
    "\n",
    "        suod_mask = self._ensemble_predict(z_rel, rule=self.flag_rule)\n",
    "        \n",
    "        inv_cov = np.linalg.pinv(self.latent_cov_inlier)\n",
    "        diff = z_rel - self.latent_mean_inlier\n",
    "        mahal_sq = np.sum(diff @ inv_cov * diff, axis=1)\n",
    "        t2_mask = mahal_sq > self.t2_threshold\n",
    "\n",
    "        x_recon = self.decoder.predict(z_sample, batch_size=batch_size)\n",
    "        recon_errors = np.sum((data - x_recon) ** 2, axis=1)\n",
    "        # Threshold is the 95th percentile of errors seen during training inliers\n",
    "        recon_threshold = np.percentile(np.sum((self._X_train[self.inlier_mask] - \n",
    "                                         self.decoder.predict(self.encoder.predict(self._X_train[self.inlier_mask])[2]))**2, axis=1), 95)\n",
    "        recon_mask = recon_errors > recon_threshold\n",
    "\n",
    "        votes = np.stack([suod_mask, t2_mask, cp_mask, recon_mask], axis=1)\n",
    "        final_outlier = np.sum(votes, axis=1) >= 2\n",
    "        \n",
    "        return final_outlier, cp_mask, suod_mask, t2_mask, recon_mask, mahal_sq, self.t2_threshold, recon_threshold\n",
    "\n",
    "    def plot_control_chart(self, data, batch_size=32, phase=\"Phase I\", show_plot=True):\n",
    "            \"\"\"\n",
    "            Generates statistical control charts for Phase I (cleaning/baseline) or Phase II (real-time monitoring).\n",
    "            \"\"\"\n",
    "            x_vals = np.arange(len(data))\n",
    "\n",
    "            if phase == \"Phase I\":\n",
    "                is_out, cp, suod, t2, rec, m2, thresh, _ = self.is_outlier(data, batch_size)\n",
    "                \n",
    "                if show_plot:\n",
    "                    plt.figure(figsize=(16, 7))\n",
    "                    # Plot continuous distance line\n",
    "                    plt.plot(x_vals, m2, color='gray', linestyle='-', linewidth=1, alpha=0.5, label='Mahalanobis²')\n",
    "                    \n",
    "                    # Markers for specific detection sources\n",
    "                    plt.plot(x_vals[is_out], m2[is_out], 'ro', markersize=8, label='Out-of-Control')\n",
    "                    plt.plot(x_vals[cp], m2[cp], 'D', markersize=4, linestyle='None', label='ChangePoint Flag')\n",
    "                    plt.plot(x_vals[suod], m2[suod], 'kx', markersize=4, linestyle='None', label='Ensemble Flag')\n",
    "                    plt.plot(x_vals[t2], m2[t2], 'g+', markersize=4, linestyle='None', label='T² Flag')\n",
    "                    plt.plot(x_vals[rec], m2[rec], 'y^', markersize=4, linestyle='None', label='Reconstruction Flag')\n",
    "                    \n",
    "                    plt.axhline(y=thresh, color='green', linestyle='--', linewidth=2, label='T² Threshold')\n",
    "                    \n",
    "                    if self.change_points is not None:\n",
    "                        for p in self.change_points:\n",
    "                            plt.axvline(p, color='magenta', linestyle=':', alpha=0.6, label='Changepoint' if p == self.change_points[0] else \"\")\n",
    "                    \n",
    "                    plt.title('VSCOUT: Phase I Control Chart')\n",
    "                    plt.xlabel('Sample Index')\n",
    "                    plt.ylabel('Mahalanobis² Value')\n",
    "                    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                return is_out\n",
    "\n",
    "            elif phase == \"Phase II\":\n",
    "                    z_m, _, _ = self.encoder.predict(data, batch_size=batch_size)\n",
    "                    z_rel = z_m[:, self.relevant_latents]\n",
    "                    diff = z_rel - self.latent_mean_inlier\n",
    "                    T2 = np.sum(diff @ np.linalg.pinv(self.latent_cov_inlier) * diff, axis=1)\n",
    "                    out_mask = T2 > self.t2_threshold\n",
    "                    \n",
    "                    if show_plot:\n",
    "                        plt.figure(figsize=(16, 7))\n",
    "                        \n",
    "                        # Consistent gray path for distances\n",
    "                        plt.plot(x_vals, T2, color='gray', linestyle='-', linewidth=1, alpha=0.5, label='Mahalanobis² (T²)')\n",
    "                        \n",
    "                        # Consistent Out-of-control marker (Red Circle)\n",
    "                        plt.plot(x_vals[out_mask], T2[out_mask], 'ro', markersize=8, label='Out-of-Control')\n",
    "                        \n",
    "                        # Consistent green threshold line\n",
    "                        plt.axhline(y=self.t2_threshold, color='green', linestyle='--', linewidth=2, label='T² Threshold')\n",
    "                        \n",
    "                        plt.title('VSCOUT: Phase II Latent Space Monitoring')\n",
    "                        plt.xlabel('Sample Index')\n",
    "                        plt.ylabel('Hotelling T² Distance')\n",
    "                        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "                        plt.grid(True, alpha=0.3)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    return out_mask\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(\"phase must be 'Phase I' or 'Phase II'\")\n",
    "\n",
    "    @property\n",
    "    def latent_mean(self): return self.latent_mean_inlier\n",
    "\n",
    "    @property\n",
    "    def latent_cov(self): return self.latent_cov_inlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "from scipy.stats import chi2\n",
    "\n",
    "class VAE:\n",
    "    def __init__(\n",
    "        self, \n",
    "        latent_dim=2, \n",
    "        encoder_layers=(64, 32), \n",
    "        decoder_layers=(32, 64),\n",
    "        learning_rate=1e-3, \n",
    "        alpha=0.05, \n",
    "        combine_rule=\"or\", \n",
    "        use_sidak=True\n",
    "    ):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.combine_rule = combine_rule  # \"or\" or \"and\"\n",
    "        self.use_sidak = use_sidak\n",
    "\n",
    "        self.vae = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "        self.latent_mean = None\n",
    "        self.latent_cov_inv = None\n",
    "        self.T2_threshold = None\n",
    "        self.SPE_threshold = None\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Build encoder and decoder\n",
    "    # ----------------------------------------------------\n",
    "    def _build(self, input_dim):\n",
    "        inputs = layers.Input(shape=(input_dim,))\n",
    "        x = inputs\n",
    "\n",
    "        for units in self.encoder_layers:\n",
    "            x = layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "        def sampling(args):\n",
    "            zm, zv = args\n",
    "            eps = tf.random.normal(shape=tf.shape(zm))\n",
    "            return zm + tf.exp(0.5 * zv) * eps\n",
    "\n",
    "        z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
    "\n",
    "        self.encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,))\n",
    "        y = latent_inputs\n",
    "        for units in self.decoder_layers:\n",
    "            y = layers.Dense(units, activation=\"relu\")(y)\n",
    "        outputs = layers.Dense(input_dim, activation=\"linear\")(y)\n",
    "        self.decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
    "\n",
    "        class VAECore(Model):\n",
    "            def __init__(self, encoder, decoder):\n",
    "                super().__init__()\n",
    "                self.encoder = encoder\n",
    "                self.decoder = decoder\n",
    "            def call(self, inputs):\n",
    "                z_mean, z_log_var, z = self.encoder(inputs)\n",
    "                return self.decoder(z)\n",
    "\n",
    "        self.vae = VAECore(self.encoder, self.decoder)\n",
    "        self.vae.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(self.learning_rate),\n",
    "            loss='mse'\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # TRAIN MODEL\n",
    "    # ----------------------------------------------------\n",
    "    def fit(self, X, epochs=50, batch_size=32, verbose=0):\n",
    "        input_dim = X.shape[1]\n",
    "        self._build(input_dim)\n",
    "\n",
    "        early_stop = callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "        self.vae.fit(X, X, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[early_stop])\n",
    "\n",
    "        z_mean, _, _ = self.encoder.predict(X, verbose=0)\n",
    "        X_recon = self.decoder.predict(z_mean, verbose=0)\n",
    "\n",
    "        self.latent_mean = np.mean(z_mean, axis=0)\n",
    "        cov = np.cov(z_mean, rowvar=False)\n",
    "        self.latent_cov_inv = np.linalg.pinv(cov)\n",
    "\n",
    "        T2_scores = np.einsum(\"ij,jk,ik->i\", z_mean - self.latent_mean, self.latent_cov_inv, z_mean - self.latent_mean)\n",
    "\n",
    "        # Compute SPE stats\n",
    "        SPE_scores = np.sum((X - X_recon) ** 2, axis=1)\n",
    "\n",
    "        # Adjust alpha using Šidák or Bonferroni\n",
    "        if self.use_sidak:\n",
    "            alpha_star = 1 - (1 - self.alpha)**(1/2)\n",
    "        else:\n",
    "            alpha_star = self.alpha / 2\n",
    "\n",
    "        # Thresholds\n",
    "        self.T2_threshold = chi2.ppf(1 - alpha_star, df=self.latent_dim)\n",
    "        self.SPE_threshold = np.quantile(SPE_scores, 1 - alpha_star)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # OUTLIER DECISION INTERFACE\n",
    "    # ----------------------------------------------------\n",
    "    def is_outlier(self, X):\n",
    "        z_mean, _, _ = self.encoder.predict(X, verbose=0)\n",
    "        X_recon = self.decoder.predict(z_mean, verbose=0)\n",
    "\n",
    "        # Stats\n",
    "        T2_scores = np.einsum(\"ij,jk,ik->i\", z_mean - self.latent_mean, self.latent_cov_inv, z_mean - self.latent_mean)\n",
    "        SPE_scores = np.sum((X - X_recon) ** 2, axis=1)\n",
    "\n",
    "        T2_flag = T2_scores > self.T2_threshold\n",
    "        SPE_flag = SPE_scores > self.SPE_threshold\n",
    "\n",
    "        if self.combine_rule == \"and\":\n",
    "            outlier = np.logical_and(T2_flag, SPE_flag)\n",
    "        else:\n",
    "            outlier = np.logical_or(T2_flag, SPE_flag)\n",
    "\n",
    "        return outlier, T2_scores, SPE_scores, self.T2_threshold, self.SPE_threshold\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # PyOD-style PREDICT\n",
    "    # ----------------------------------------------------\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return 1 for outlier, 0 for inlier.\"\"\"\n",
    "        out, *_ = self.is_outlier(X)\n",
    "        return out.astype(int)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # PyOD-style SCORE FUNCTION \n",
    "    # ----------------------------------------------------\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Return an anomaly score: higher = more anomalous.\"\"\"\n",
    "\n",
    "        z_mean, _, _ = self.encoder.predict(X, verbose=0)\n",
    "        X_recon = self.decoder.predict(z_mean, verbose=0)\n",
    "\n",
    "        T2_scores = np.einsum(\"ij,jk,ik->i\", z_mean - self.latent_mean, self.latent_cov_inv, z_mean - self.latent_mean)\n",
    "        SPE_scores = np.sum((X - X_recon) ** 2, axis=1)\n",
    "\n",
    "        # Standardize both to comparable scale\n",
    "        T2_norm = (T2_scores - np.min(T2_scores)) / (np.ptp(T2_scores) + 1e-8)\n",
    "        SPE_norm = (SPE_scores - np.min(SPE_scores)) / (np.ptp(SPE_scores) + 1e-8)\n",
    "\n",
    "        # Combined score\n",
    "        score = np.maximum(T2_norm, SPE_norm)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88840de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.suod import SUOD  # <-- ADD THIS\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# .mat DATA LOADER\n",
    "# ==========================================================\n",
    "def load_mat_dataset(filepath):\n",
    "    d = scipy.io.loadmat(filepath)\n",
    "\n",
    "    for key in [\"X\", \"x\", \"data\"]:\n",
    "        if key in d and isinstance(d[key], np.ndarray):\n",
    "            X = d[key]\n",
    "            break\n",
    "    else:\n",
    "        arrays = [\n",
    "            v for v in d.values()\n",
    "            if isinstance(v, np.ndarray) and v.ndim >= 2\n",
    "        ]\n",
    "        if not arrays:\n",
    "            raise ValueError(\"No 2D array found in MAT file.\")\n",
    "        X = max(arrays, key=lambda a: a.size)\n",
    "\n",
    "    for key in [\"y\", \"Y\", \"label\", \"labels\"]:\n",
    "        if key in d and isinstance(d[key], np.ndarray):\n",
    "            y = d[key].ravel()\n",
    "            break\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    if y is not None:\n",
    "        if np.min(y) < 0:\n",
    "            y = (y == 1).astype(int)\n",
    "        if set(np.unique(y)) not in [{0, 1}, {1}]:\n",
    "            # convert to 0/1 by majority rule\n",
    "            y = (y == np.max(y)).astype(int)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# METRIC COMPUTATION\n",
    "# ==========================================================\n",
    "def compute_metrics(y_true, is_out, scores):\n",
    "    \"\"\"\n",
    "    is_out = 1 for anomaly, 0 for normal\n",
    "    \"\"\"\n",
    "    recall = np.mean(is_out[y_true == 1]) if np.sum(y_true == 1) > 0 else 0.0\n",
    "    precision = np.mean(y_true[is_out == 1] == 1) if np.sum(is_out == 1) > 0 else 0.0\n",
    "    fpr = np.mean(is_out[y_true == 0]) if np.sum(y_true == 0) > 0 else 0.0\n",
    "    inlier_ret = np.mean(is_out[y_true == 0] == 0) if np.sum(y_true == 0) > 0 else 0.0\n",
    "    auroc = roc_auc_score(y_true, scores) if len(np.unique(y_true)) == 2 else 0.5\n",
    "    return recall, precision, fpr, inlier_ret, auroc\n",
    "\n",
    "\n",
    "def sample_from_dataset(X, y, n_samples=500, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    idx = rng.choice(len(X), size=n_samples, replace=False)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# BENCHMARK FUNCTION\n",
    "# ==========================================================\n",
    "def run_benchmark_on_file(filepath, models, scaler=StandardScaler(), runs=10):\n",
    "\n",
    "    dataset_name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    X, y = load_mat_dataset(filepath)\n",
    "\n",
    "\n",
    "    print(np.unique(y, return_counts=True))\n",
    "\n",
    "    if y is None:\n",
    "        raise ValueError(f\"No labels found in {filepath}\")\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for mdl_name, mdl_fn in models.items():\n",
    "\n",
    "        # Store the metric history across runs\n",
    "        rec_list, prec_list, fpr_list, inret_list, auc_list = [], [], [], [], []\n",
    "\n",
    "        for r in range(runs):\n",
    "            model = mdl_fn()\n",
    "\n",
    "            if mdl_name.upper() == \"VSCOUT\":\n",
    "                model.fit(X_scaled, epochs=40, batch_size=64, verbose=0)\n",
    "\n",
    "                out = model.is_outlier(X_scaled)\n",
    "                if isinstance(out, tuple):\n",
    "                    is_out = np.array(out[0]).astype(int)\n",
    "                else:\n",
    "                    is_out = np.array(out).astype(int)\n",
    "\n",
    "                # Scores\n",
    "                if hasattr(model, \"decision_function\"):\n",
    "                    scores = model.decision_function(X_scaled)\n",
    "                else:\n",
    "                    scores = is_out.astype(float)\n",
    "\n",
    "            else:\n",
    "                model.fit(X_scaled)\n",
    "\n",
    "                if hasattr(model, \"predict\"):\n",
    "                    is_out = model.predict(X_scaled).astype(int)\n",
    "                else:\n",
    "                    raise ValueError(f\"No usable predict() for {mdl_name}\")\n",
    "\n",
    "                if hasattr(model, \"decision_function\"):\n",
    "                    scores = model.decision_function(X_scaled)\n",
    "                else:\n",
    "                    scores = is_out.astype(float)\n",
    "\n",
    "            # Compute metrics\n",
    "            recall, precision, fpr, inlier_ret, auroc = compute_metrics(y, is_out, scores)\n",
    "\n",
    "            rec_list.append(recall)\n",
    "            prec_list.append(precision)\n",
    "            fpr_list.append(fpr)\n",
    "            inret_list.append(inlier_ret)\n",
    "            auc_list.append(auroc)\n",
    "\n",
    "        # Store means + stds\n",
    "        rows.append(dict(\n",
    "            Dataset=dataset_name,\n",
    "            Model=mdl_name,\n",
    "            Recall_mean=np.mean(rec_list), Recall_std=np.std(rec_list),\n",
    "            Precision_mean=np.mean(prec_list), Precision_std=np.std(prec_list),\n",
    "            FPR_mean=np.mean(fpr_list), FPR_std=np.std(fpr_list),\n",
    "            InlierRetention_mean=np.mean(inret_list), InlierRetention_std=np.std(inret_list),\n",
    "            AUROC_mean=np.mean(auc_list), AUROC_std=np.std(auc_list),\n",
    "        ))\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# LATEX TABLE GENERATOR\n",
    "# ==========================================================\n",
    "def make_latex_table(results_df, dataset_sizes):\n",
    "    \"\"\"\n",
    "    results_df contains means and std deviations.\n",
    "    dataset_sizes = dict like {\"ionosphere\": \"(351, 34)\", ...}\n",
    "    \"\"\"\n",
    "\n",
    "    def fmt(mean, std):\n",
    "        return f\"{mean:.4f} ({std:.2f})\"\n",
    "\n",
    "    L = []\n",
    "    L.append(\"\\\\begin{table}[H]\")\n",
    "    L.append(\"\\\\centering\")\n",
    "    L.append(\"\\\\scriptsize\")\n",
    "    L.append(\"\\\\caption{Comparison on Benchmark Outlier-Detection Datasets.}\")\n",
    "    L.append(\"\\\\label{tab:benchmark-results}\")\n",
    "    L.append(\"\\\\resizebox{0.71\\\\textwidth}{!}{\")\n",
    "    L.append(\"\\\\begin{tabular}{lllccccc}\")\n",
    "    L.append(\"\\\\toprule\")\n",
    "    L.append(\"Dataset & Size $(n,p)$ & Model & Recall & Precision & FPR & Inlier Retention & AUROC \\\\\\\\\")\n",
    "    L.append(\"\\\\midrule\")\n",
    "\n",
    "    for dataset in results_df[\"Dataset\"].unique():\n",
    "\n",
    "        df_sub = results_df[results_df[\"Dataset\"] == dataset]\n",
    "        first = True\n",
    "        size = dataset_sizes.get(dataset, \"(?,?)\")\n",
    "\n",
    "        for _, row in df_sub.iterrows():\n",
    "\n",
    "            dataset_cell = dataset if first else \"\"\n",
    "            size_cell = size if first else \"\"\n",
    "            first = False\n",
    "\n",
    "            L.append(\n",
    "                f\"{dataset_cell} & {size_cell} & {row['Model']} & \"\n",
    "                f\"{fmt(row['Recall_mean'], row['Recall_std'])} & \"\n",
    "                f\"{fmt(row['Precision_mean'], row['Precision_std'])} & \"\n",
    "                f\"{fmt(row['FPR_mean'], row['FPR_std'])} & \"\n",
    "                f\"{fmt(row['InlierRetention_mean'], row['InlierRetention_std'])} & \"\n",
    "                f\"{fmt(row['AUROC_mean'], row['AUROC_std'])} \\\\\\\\\"\n",
    "            )\n",
    "\n",
    "        L.append(\"\\\\midrule\")\n",
    "\n",
    "    L[-1] = \"\\\\bottomrule\"  # Replace last midrule\n",
    "    L.append(\"\\\\end{tabular}\")\n",
    "    L.append(\"}\")\n",
    "    L.append(\"\\\\end{table}\")\n",
    "\n",
    "    return \"\\n\".join(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc576b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files = [\n",
    "    \"/datasets/arrhythmia.mat\",\n",
    "    \"/datasets/cardio.mat\",\n",
    "    \"datasets/glass.mat\",\n",
    "    \"/datasets/ionosphere.mat\",\n",
    "    \"/datasets/letter.mat\",\n",
    "    \"/datasets/lympho.mat\",\n",
    "    \"/datasets/mnist.mat\",\n",
    "    \"/datasets/musk.mat\",\n",
    "    \"/datasets/optdigits.mat\",\n",
    "    \"/datasets/pendigits.mat\"\n",
    "]\n",
    "models = {\n",
    "    \"VSCOUT\": lambda: VSCOUT(),\n",
    "    \"VAE\": lambda: VAE(alpha = 0.05),\n",
    "    \"IForest\": lambda: IForest(contamination=0.05),\n",
    "    \"LOF\": lambda: LOF(contamination = 0.05),\n",
    "    \"ECOD\": lambda: ECOD(contamination=0.05),\n",
    "    \"KNN\": lambda: KNN(contamination=0.05),\n",
    "    \"SUOD\": lambda: SUOD(\n",
    "        base_estimators=[\n",
    "            IForest(contamination=0.05),\n",
    "            LOF(contamination=0.05),\n",
    "            ECOD(contamination=0.05),\n",
    "            KNN(contamination=0.05),\n",
    "        ],\n",
    "        n_jobs=1,\n",
    "        combination=\"average\",\n",
    "        contamination=0.05\n",
    "    )\n",
    "}\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for f in mat_files:\n",
    "    print(\"Running:\", f)\n",
    "    rows = run_benchmark_on_file(f, models, runs=30) ## Increased runs greater than 30 for better statistics\n",
    "    all_rows.extend(rows)\n",
    "\n",
    "results_df = pd.DataFrame(all_rows)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
